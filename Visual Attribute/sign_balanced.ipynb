{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "version": "3.6.1",
      "name": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python"
    },
    "anaconda-cloud": {}
  },
  "cells": [
    {
      "outputs": [],
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed May  1 21:14:37 2019\n",
        "\n",
        "@author: Sonu\n",
        "\"\"\"\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/my_drive/')\n",
        "#!ls\n",
        "#!pwd\n",
        "#cd my_drive\n",
        "#cd My\\ Drive\n",
        "#cd sign_augment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "#import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D,Dropout\n",
        "from keras.layers import Activation, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from custom_metric import roc_auc_score,fmeasure\n",
        "#from keras.models import Model \n",
        "from itertools import islice\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import load_model\n",
        "def read(filename):\n",
        "    df = pd.read_csv(filename, nrows =3096)\n",
        "    print(df.head())\n",
        "    label = df['sign']\n",
        "    print(label.value_counts())\n",
        "    filename=df['fname']\n",
        "    \n",
        "    imgpath = np.empty(len(filename), dtype=object)\n",
        "    images =  np.empty(len(filename), dtype=object)\n",
        "    #server path - /home/jpstud1/Desktop/img/new/new_Augment_Images\n",
        "    for n in range(0, len(filename)):\n",
        "        imgpath[n] = filename[n]\n",
        "        images[n] = cv2.imread(imgpath[n])\n",
        "        images[n] = cv2.resize(images[n], (400, 400))\n",
        "        images[n] = images[n].astype('float32')\n",
        "        images[n] /= 255\n",
        "        #data.append(images[n])\n",
        "        \n",
        "    \n",
        "    return images,label\n",
        "    \n",
        "train_data,train_target=read('sign_balanced.csv')\n",
        "#csv path - /home/jpstud1/Desktop/img/new/new_Augment_Images/photo_balanced.csv\n",
        "\n",
        "img_rows=400\n",
        "img_cols=400\n",
        "data=[]\n",
        "\n",
        "for img in train_data:\n",
        "    data.append(img)\n",
        "\n",
        "def split_validation_set_with_hold_out(train, target, test_size):\n",
        "    random_state = 51\n",
        "    train, X_test, target, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
        "    X_train, X_holdout, y_train, y_holdout = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, X_holdout, y_train, y_test, y_holdout    \n",
        "\n",
        " \n",
        "data_train, data_test, data_holdout, label_train, label_test, label_holdout = split_validation_set_with_hold_out(data,train_target, 0.2)\n",
        "\n",
        "\n",
        "print(len(data_train))\n",
        "print(len(data_test))\n",
        "print(len(data_holdout))\n",
        "print(len(label_train))\n",
        "print(len(label_test))\n",
        "print(len(label_holdout))\n",
        "\n",
        "\n",
        "def gen():\n",
        "    print('generator initiated')\n",
        "    i=1\n",
        "    while True: \n",
        "        for i in range(0,110):\n",
        "            iteru = []\n",
        "            iteru1 = []\n",
        "            itera = islice(data_train, i*18, (i+1)*18)\n",
        "            itera1 = islice(label_train, i*18, (i+1)*18)\n",
        "            for img in itera:\n",
        "                iteru.append(img)\n",
        "            for img in itera1:\n",
        "                iteru1.append(img)\n",
        "                #print(\"img\",img)\n",
        "            #print(\"i\",i)    \n",
        "            yield np.array(iteru), np.array(iteru1)\n",
        "\n",
        "\n",
        "def gen1():\n",
        "    print('generator initiated')\n",
        "    i=1\n",
        "    while True: \n",
        "        for i in range(0,31):\n",
        "            iteru = []\n",
        "            iteru1 = []\n",
        "            itera = islice(data_holdout, i*16, (i+1)*16)\n",
        "            itera1 = islice(label_holdout, i*16, (i+1)*16)\n",
        "            for img in itera:\n",
        "                iteru.append(img)\n",
        "            for img in itera1:\n",
        "                iteru1.append(img)\n",
        "                #print(\"img\",img)\n",
        "            #print(\"i\",i)    \n",
        "            yield np.array(iteru), np.array(iteru1)\n",
        "      \n",
        "\n",
        "def gen2():\n",
        "    print('generator initiated')\n",
        "    i=1\n",
        "    while True: \n",
        "        for i in range(0,31):\n",
        "            iteru = []\n",
        "            iteru1 = []\n",
        "            itera = islice(data_test, i*20, (i+1)*20)\n",
        "            itera1 = islice(label_test, i*20, (i+1)*20)\n",
        "            for img in itera:\n",
        "                iteru.append(img)\n",
        "            for img in itera1:\n",
        "                iteru1.append(img)\n",
        "                #print(\"img\",img)\n",
        "            #print(\"i\",i)    \n",
        "            yield np.array(iteru), np.array(iteru1)\n",
        "  \n",
        "batch_size_train = 18\n",
        "batch_size_holdout = 16\n",
        "batch_size_test = 20\n",
        "#nb_classes = 11\n",
        "train_batch_length = len(label_train)\n",
        "holdout_batch_length= len(label_holdout)\n",
        "test_batch_length = len(label_test)\n",
        "count=train_batch_length/batch_size_train\n",
        "count1=holdout_batch_length/batch_size_holdout\n",
        "count2=test_batch_length/batch_size_test\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_rows, img_cols)\n",
        "else:\n",
        "    input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(8, (3, 3), input_shape=input_shape))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(8, (3, 3)))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.35))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3)))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.45))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.65))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[fmeasure])\n",
        "\n",
        "\n",
        "tr_gen = gen()\n",
        "tr_gen1 = gen1()\n",
        "tr_gen2 = gen2()\n",
        "m_check=keras.callbacks.ModelCheckpoint(filepath='./sign_augment500.h5',monitor='val_fmeasure', verbose=0, save_best_only=True, mode='max')\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.2, patience=4, verbose=1, min_lr=0.0001 )\n",
        "history=model.fit_generator(generator=tr_gen, steps_per_epoch=count, nb_epoch=80, validation_data=tr_gen1, validation_steps=count1, max_queue_size=2,callbacks=[m_check])\n",
        "\n",
        "#model.fit(np.array(data_train),np.array(label_train), batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(np.array(data_test),np.array(label_test)))\n",
        "score = model.evaluate_generator(generator=tr_gen2,steps=count2,max_queue_size=2, verbose=0)\n",
        "print('Score: ', score)\n",
        "score = model.evaluate_generator(generator=tr_gen1,steps=count1,max_queue_size=2, verbose=0)\n",
        "print('Score holdout: ', score)\n",
        "\n",
        "print(\"With best model\")\n",
        "file_path='./sign_augment500.h5'\n",
        "model1 = load_model(file_path,custom_objects={'fmeasure': fmeasure})\n",
        "score1 = model1.evaluate_generator(generator=tr_gen2,steps=count2,max_queue_size=2, verbose=0)\n",
        "print('Score: ', score1)\n",
        "score2 = model1.evaluate_generator(generator=tr_gen1,steps=count1,max_queue_size=2, verbose=0)\n",
        "print('Score holdout: ', score2)"
      ],
      "cell_type": "code",
      "execution_count": null
    }
  ],
  "nbformat_minor": 1,
  "nbformat": 4
}